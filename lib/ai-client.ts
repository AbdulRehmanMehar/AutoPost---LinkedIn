/**
 * Groq AI Client with MongoDB-backed Usage Tracking
 * 
 * Features:
 * 1. Tracks token usage per model in MongoDB (persists across restarts)
 * 2. Proactively switches models before hitting limits
 * 3. Falls back to other models on 429 errors
 * 4. Respects both daily AND per-minute limits
 * 5. Uses actual Groq dashboard limits
 */

import OpenAI from 'openai';
import AIUsage, {
  GROQ_MODEL_LIMITS,
  MODEL_PRIORITY,
  FAST_MODEL_PRIORITY,
  getDateKey,
  getMinuteKey,
  type ModelLimits,
  type IAIUsage,
} from './models/AIUsage';

// Safety threshold - switch models at this % of limit
const DAILY_THRESHOLD = 0.90;   // 90% of daily limit
const MINUTE_THRESHOLD = 0.80;  // 80% of minute limit (tighter to avoid bursting)

// In-memory cache for current minute (reduces DB calls for rate limiting)
interface MinuteCache {
  minute: Date;
  usage: Map<string, { tokens: number; requests: number }>;
}
let minuteCache: MinuteCache = {
  minute: getMinuteKey(),
  usage: new Map(),
};

// In-memory cache for daily usage (refreshed periodically)
interface DailyCache {
  date: Date;
  usage: Map<string, { tokens: number; requests: number; rateLimitHits: number }>;
  lastRefresh: number;
}
let dailyCache: DailyCache = {
  date: getDateKey(),
  usage: new Map(),
  lastRefresh: 0,
};

const CACHE_TTL = 5000; // Refresh daily cache every 5 seconds

// ============================================
// MongoDB-backed Usage Tracking
// ============================================

/**
 * Refresh daily cache from MongoDB
 */
async function refreshDailyCache(): Promise<void> {
  const now = Date.now();
  const today = getDateKey();
  
  // Check if cache is stale or date changed
  if (
    dailyCache.date.getTime() === today.getTime() && 
    now - dailyCache.lastRefresh < CACHE_TTL
  ) {
    return;
  }
  
  try {
    const records = await AIUsage.find({ date: today }).lean();
    
    dailyCache = {
      date: today,
      usage: new Map(),
      lastRefresh: now,
    };
    
    for (const record of records) {
      dailyCache.usage.set(record.modelName, {
        tokens: record.tokensUsed,
        requests: record.requestCount,
        rateLimitHits: record.rateLimitHits,
      });
    }
  } catch (error) {
    console.warn('[AI] Error refreshing cache:', error);
  }
}

/**
 * Get usage from cache (with fallback to DB)
 */
async function getDailyUsage(model: string): Promise<{ tokens: number; requests: number; rateLimitHits: number }> {
  await refreshDailyCache();
  return dailyCache.usage.get(model) || { tokens: 0, requests: 0, rateLimitHits: 0 };
}

/**
 * Update minute cache
 */
function updateMinuteCache(model: string, tokens: number): void {
  const currentMinute = getMinuteKey();
  
  // Reset cache if minute changed
  if (minuteCache.minute.getTime() !== currentMinute.getTime()) {
    minuteCache = {
      minute: currentMinute,
      usage: new Map(),
    };
  }
  
  const current = minuteCache.usage.get(model) || { tokens: 0, requests: 0 };
  minuteCache.usage.set(model, {
    tokens: current.tokens + tokens,
    requests: current.requests + 1,
  });
}

/**
 * Get minute usage from cache
 */
function getMinuteUsage(model: string): { tokens: number; requests: number } {
  const currentMinute = getMinuteKey();
  
  if (minuteCache.minute.getTime() !== currentMinute.getTime()) {
    minuteCache = {
      minute: currentMinute,
      usage: new Map(),
    };
  }
  
  return minuteCache.usage.get(model) || { tokens: 0, requests: 0 };
}

/**
 * Record usage in MongoDB (async, non-blocking)
 */
async function recordUsage(model: string, tokens: number, success: boolean = true): Promise<void> {
  const today = getDateKey();
  const currentMinute = getMinuteKey();
  
  // Update in-memory caches immediately
  updateMinuteCache(model, tokens);
  const cached = dailyCache.usage.get(model) || { tokens: 0, requests: 0, rateLimitHits: 0 };
  dailyCache.usage.set(model, {
    tokens: cached.tokens + tokens,
    requests: cached.requests + 1,
    rateLimitHits: cached.rateLimitHits + (success ? 0 : 0),
  });
  
  try {
    // Update or create daily record
    const result = await AIUsage.findOneAndUpdate(
      { date: today, modelName: model },
      {
        $inc: { 
          tokensUsed: tokens, 
          requestCount: 1,
          errorCount: success ? 0 : 1,
        },
        $set: { lastUpdated: new Date() },
      },
      { upsert: true, new: true }
    );
    
    // Log usage
    const limits = GROQ_MODEL_LIMITS[model];
    if (limits?.tokensPerDay) {
      const percentUsed = ((result.tokensUsed / limits.tokensPerDay) * 100).toFixed(1);
      console.log(`[AI] ${model}: ${result.tokensUsed.toLocaleString()}/${limits.tokensPerDay.toLocaleString()} tokens (${percentUsed}%)`);
    } else {
      console.log(`[AI] ${model}: ${result.tokensUsed.toLocaleString()} tokens (no daily limit)`);
    }
  } catch (error) {
    console.error('[AI] Error recording usage:', error);
  }
}

/**
 * Record a rate limit hit and mark model as exhausted
 */
async function recordRateLimitHit(model: string): Promise<void> {
  const today = getDateKey();
  const limits = GROQ_MODEL_LIMITS[model];
  
  // Mark model as exhausted in cache (set tokens to limit to prevent future selection)
  const exhaustedTokens = limits?.tokensPerDay || 100000;
  dailyCache.usage.set(model, {
    tokens: exhaustedTokens,  // Mark as fully used
    requests: limits?.requestsPerDay || 1000,
    rateLimitHits: (dailyCache.usage.get(model)?.rateLimitHits || 0) + 1,
  });
  
  console.log(`[AI] Marked ${model} as exhausted (${exhaustedTokens} tokens)`);
  
  try {
    await AIUsage.updateOne(
      { date: today, modelName: model },
      { 
        $inc: { rateLimitHits: 1 },
        $set: { 
          lastUpdated: new Date(),
          tokensUsed: exhaustedTokens,  // Mark as exhausted in DB too
        },
      },
      { upsert: true }
    );
  } catch (error) {
    console.error('[AI] Error recording rate limit:', error);
  }
}

// ============================================
// Model Selection
// ============================================

/**
 * Check if a model has capacity for a request
 */
async function hasCapacity(
  model: string, 
  estimatedTokens: number = 2000
): Promise<{ 
  available: boolean; 
  reason?: string;
  dailyUsed?: number;
  dailyLimit?: number | null;
}> {
  const limits = GROQ_MODEL_LIMITS[model];
  if (!limits) {
    return { available: true };  // Unknown model, try it
  }
  
  // Check minute limit (from in-memory cache - instant)
  const minuteUsage = getMinuteUsage(model);
  const projectedMinuteTokens = minuteUsage.tokens + estimatedTokens;
  
  if (projectedMinuteTokens > limits.tokensPerMinute * MINUTE_THRESHOLD) {
    return { 
      available: false, 
      reason: `Minute tokens: ${minuteUsage.tokens}/${limits.tokensPerMinute}` 
    };
  }
  
  if (minuteUsage.requests >= limits.requestsPerMinute * MINUTE_THRESHOLD) {
    return { 
      available: false, 
      reason: `Minute requests: ${minuteUsage.requests}/${limits.requestsPerMinute}` 
    };
  }
  
  // Check daily limit (null = no limit for compound models)
  if (limits.tokensPerDay !== null) {
    const usage = await getDailyUsage(model);
    const projectedDailyTokens = usage.tokens + estimatedTokens;
    
    if (projectedDailyTokens > limits.tokensPerDay * DAILY_THRESHOLD) {
      return { 
        available: false, 
        reason: `Daily tokens: ${usage.tokens.toLocaleString()}/${limits.tokensPerDay.toLocaleString()}`,
        dailyUsed: usage.tokens,
        dailyLimit: limits.tokensPerDay,
      };
    }
    
    if (usage.requests >= limits.requestsPerDay * DAILY_THRESHOLD) {
      return { 
        available: false, 
        reason: `Daily requests: ${usage.requests}/${limits.requestsPerDay}` 
      };
    }
    
    return { 
      available: true,
      dailyUsed: usage.tokens,
      dailyLimit: limits.tokensPerDay,
    };
  }
  
  // No daily limit (compound models)
  return { available: true, dailyLimit: null };
}

/**
 * Calculate usage percentage for a model
 * Returns 0 for unlimited models, 1+ for exhausted models
 */
async function getUsagePercent(model: string): Promise<number> {
  const limits = GROQ_MODEL_LIMITS[model];
  if (!limits) return 0;
  
  // Compound models have no daily limit - return 0 (always available)
  if (limits.tokensPerDay === null) {
    return 0;
  }
  
  const usage = await getDailyUsage(model);
  return usage.tokens / limits.tokensPerDay;
}

/**
 * SMART MODEL SELECTION
 * 
 * Strategy: Always pick the model with the LOWEST usage percentage.
 * This distributes load evenly across all models and maximizes total capacity.
 * 
 * For models with no daily limit (compound), they're treated as 0% usage
 * but we prefer "real" models when they have capacity.
 */
async function getAvailableModel(preferFast: boolean = false, estimatedTokens: number = 2000): Promise<string> {
  const allModels = preferFast ? FAST_MODEL_PRIORITY : MODEL_PRIORITY;
  
  // Calculate usage % and capacity for all models
  const modelStats: Array<{
    model: string;
    usagePercent: number;
    hasCapacity: boolean;
    isUnlimited: boolean;
    reason?: string;
  }> = [];
  
  for (const model of allModels) {
    const limits = GROQ_MODEL_LIMITS[model];
    const isUnlimited = limits?.tokensPerDay === null;
    const { available, reason } = await hasCapacity(model, estimatedTokens);
    const usagePercent = await getUsagePercent(model);
    
    modelStats.push({
      model,
      usagePercent,
      hasCapacity: available,
      isUnlimited,
      reason,
    });
  }
  
  // Filter to only models with capacity
  const availableModels = modelStats.filter(m => m.hasCapacity);
  
  if (availableModels.length === 0) {
    // No models available - find the one with lowest usage (might still work)
    console.warn('[AI] All models near capacity!');
    
    // Prefer unlimited models as last resort
    const unlimited = modelStats.find(m => m.isUnlimited);
    if (unlimited) {
      console.log(`[AI] Falling back to ${unlimited.model} (no daily limit)`);
      return unlimited.model;
    }
    
    // Otherwise pick lowest usage
    const sorted = [...modelStats].sort((a, b) => a.usagePercent - b.usagePercent);
    console.log(`[AI] Using ${sorted[0].model} (${(sorted[0].usagePercent * 100).toFixed(1)}% used)`);
    return sorted[0].model;
  }
  
  // Split into limited and unlimited
  const limitedModels = availableModels.filter(m => !m.isUnlimited);
  const unlimitedModels = availableModels.filter(m => m.isUnlimited);
  
  // Prefer limited models with lowest usage (save unlimited for when we need them)
  if (limitedModels.length > 0) {
    // Sort by usage percentage (lowest first)
    limitedModels.sort((a, b) => a.usagePercent - b.usagePercent);
    const selected = limitedModels[0];
    
    // Only log if switching or notable
    if (selected.usagePercent > 0.5) {
      console.log(`[AI] Selected ${selected.model} (${(selected.usagePercent * 100).toFixed(1)}% used - lowest available)`);
    }
    
    return selected.model;
  }
  
  // Only unlimited models available
  const selected = unlimitedModels[0];
  console.log(`[AI] Using ${selected.model} (unlimited - all limited models exhausted)`);
  return selected.model;
}

// ============================================
// Groq Client
// ============================================

const groqClient = new OpenAI({
  apiKey: process.env.GROQ_API_KEY,
  baseURL: 'https://api.groq.com/openai/v1',
});

/**
 * Parse retry-after from error or headers
 */
function parseRetryAfter(error: unknown): number | undefined {
  if (error && typeof error === 'object') {
    // Check for retry-after in headers
    const headers = (error as { headers?: { get?: (key: string) => string | null } }).headers;
    if (headers?.get) {
      const retryAfter = headers.get('retry-after');
      if (retryAfter) {
        return parseInt(retryAfter, 10);
      }
    }
    
    // Parse from error message
    const message = (error as { message?: string }).message || '';
    const match = message.match(/try again in (\d+(?:\.\d+)?)(s|m|h)/i);
    if (match) {
      const value = parseFloat(match[1]);
      const unit = match[2].toLowerCase();
      if (unit === 'm') return Math.ceil(value * 60);
      if (unit === 'h') return Math.ceil(value * 3600);
      return Math.ceil(value);
    }
  }
  return undefined;
}

/**
 * Estimate tokens for a request (rough heuristic: 4 chars â‰ˆ 1 token)
 */
function estimateTokens(messages: Array<{ content: string }>, maxTokens: number): number {
  const inputTokens = messages.reduce((sum, m) => sum + Math.ceil(m.content.length / 4), 0);
  return inputTokens + maxTokens;
}

// ============================================
// Main API
// ============================================

export interface ChatCompletionOptions {
  messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>;
  temperature?: number;
  maxTokens?: number;
  preferFast?: boolean;  // Prefer faster/smaller models
  maxRetries?: number;   // Max number of model switches on rate limit
}

export interface ChatCompletionResult {
  content: string | null;
  model: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

/**
 * Create a chat completion with automatic model rotation
 */
export async function createChatCompletion(
  options: ChatCompletionOptions
): Promise<ChatCompletionResult> {
  const { 
    messages, 
    temperature = 0.7, 
    maxTokens = 1000, 
    preferFast = false,
    maxRetries = 3 
  } = options;
  
  const estimatedTokens = estimateTokens(messages, maxTokens);
  let lastError: Error | null = null;
  const triedModels = new Set<string>();
  
  for (let attempt = 0; attempt < maxRetries + 1; attempt++) {
    // Get model with capacity
    const model = await getAvailableModel(preferFast, estimatedTokens);
    
    // Don't retry same model
    if (triedModels.has(model)) {
      break;  // All available models tried
    }
    triedModels.add(model);
    
    try {
      console.log(`[AI] Using ${model} (attempt ${attempt + 1}/${maxRetries + 1})`);
      
      const response = await groqClient.chat.completions.create({
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
      });
      
      // Record actual usage
      if (response.usage) {
        await recordUsage(model, response.usage.total_tokens, true);
      }
      
      const content = response.choices[0]?.message?.content || null;
      const finishReason = response.choices[0]?.finish_reason;
      
      // Debug: Log finish reason and content info
      console.log(`[AI] Response: finish_reason=${finishReason}, content_length=${content?.length || 0}`);
      
      // Debug: Log if content is empty despite successful response
      if (!content && response.choices.length > 0) {
        console.warn(`[AI] Warning: Response has choices but content is empty/null`);
        console.warn(`[AI] Choices:`, JSON.stringify(response.choices, null, 2));
      }
      
      return {
        content,
        model,
        usage: response.usage ? {
          promptTokens: response.usage.prompt_tokens,
          completionTokens: response.usage.completion_tokens,
          totalTokens: response.usage.total_tokens,
        } : undefined,
      };
    } catch (error) {
      lastError = error as Error;
      
      // Check if it's a rate limit error
      const status = (error as { status?: number }).status;
      const code = (error as { code?: string }).code;
      
      if (status === 429 || code === 'rate_limit_exceeded') {
        await recordRateLimitHit(model);
        console.log(`[AI] Rate limited on ${model}, switching to next model...`);
        continue;
      }
      
      // For other errors, record and throw
      await recordUsage(model, estimatedTokens, false);
      throw error;
    }
  }
  
  // All retries exhausted
  throw lastError || new Error('All models at capacity');
}

/**
 * Convenience function for simple completions
 */
export async function complete(
  prompt: string,
  options?: Partial<ChatCompletionOptions>
): Promise<string | null> {
  const result = await createChatCompletion({
    messages: [{ role: 'user', content: prompt }],
    ...options,
  });
  return result.content;
}

/**
 * Convenience function for system + user prompt
 */
export async function completeWithSystem(
  systemPrompt: string,
  userPrompt: string,
  options?: Partial<ChatCompletionOptions>
): Promise<string | null> {
  const result = await createChatCompletion({
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt },
    ],
    ...options,
  });
  return result.content;
}

/**
 * Get current usage status for all models (from MongoDB)
 */
export async function getUsageStatus(): Promise<Record<string, { 
  tokensUsed: number; 
  tokensLimit: number | null; 
  requestsUsed: number;
  requestsLimit: number;
  percentUsed: number | null;
  available: boolean;
  rateLimitHits: number;
}>> {
  await refreshDailyCache();
  const status: Record<string, any> = {};
  
  for (const model of MODEL_PRIORITY) {
    const limits = GROQ_MODEL_LIMITS[model];
    if (!limits) continue;
    
    const usage = await getDailyUsage(model);
    const { available } = await hasCapacity(model);
    
    status[model] = {
      tokensUsed: usage.tokens,
      tokensLimit: limits.tokensPerDay,
      requestsUsed: usage.requests,
      requestsLimit: limits.requestsPerDay,
      percentUsed: limits.tokensPerDay 
        ? Math.round((usage.tokens / limits.tokensPerDay) * 1000) / 10
        : null,
      available,
      rateLimitHits: usage.rateLimitHits,
    };
  }
  
  return status;
}

/**
 * Get total available capacity across all models
 */
export async function getTotalCapacity(): Promise<{ 
  totalUsed: number; 
  totalLimit: number; 
  percentUsed: number;
  availableModels: string[];
  unlimitedModels: string[];
}> {
  const status = await getUsageStatus();
  let totalUsed = 0;
  let totalLimit = 0;
  const availableModels: string[] = [];
  const unlimitedModels: string[] = [];
  
  for (const [model, s] of Object.entries(status)) {
    if (s.tokensLimit) {
      totalUsed += s.tokensUsed || 0;
      totalLimit += s.tokensLimit;
    } else {
      unlimitedModels.push(model);
    }
    if (s.available) {
      availableModels.push(model);
    }
  }
  
  return {
    totalUsed,
    totalLimit,
    percentUsed: Math.round((totalUsed / totalLimit) * 1000) / 10,
    availableModels,
    unlimitedModels,
  };
}

/**
 * Get which model would be selected for a request (for debugging/monitoring)
 */
export async function getSelectedModel(preferFast: boolean = false): Promise<{
  model: string;
  usagePercent: number;
  reasoning: string;
  allModels: Array<{
    model: string;
    usagePercent: number;
    hasCapacity: boolean;
    tokensUsed: number;
    tokensLimit: number | null;
  }>;
}> {
  const allModels = preferFast ? FAST_MODEL_PRIORITY : MODEL_PRIORITY;
  const modelDetails: Array<{
    model: string;
    usagePercent: number;
    hasCapacity: boolean;
    tokensUsed: number;
    tokensLimit: number | null;
  }> = [];
  
  for (const model of allModels) {
    const limits = GROQ_MODEL_LIMITS[model];
    const usage = await getDailyUsage(model);
    const { available } = await hasCapacity(model);
    const usagePercent = limits?.tokensPerDay 
      ? (usage.tokens / limits.tokensPerDay) * 100
      : 0;
    
    modelDetails.push({
      model,
      usagePercent: Math.round(usagePercent * 10) / 10,
      hasCapacity: available,
      tokensUsed: usage.tokens,
      tokensLimit: limits?.tokensPerDay ?? null,
    });
  }
  
  // Sort by usage % (same logic as getAvailableModel)
  const available = modelDetails.filter(m => m.hasCapacity && m.tokensLimit !== null);
  const unlimited = modelDetails.filter(m => m.hasCapacity && m.tokensLimit === null);
  
  let selected: typeof modelDetails[0];
  let reasoning: string;
  
  if (available.length > 0) {
    available.sort((a, b) => a.usagePercent - b.usagePercent);
    selected = available[0];
    reasoning = `Lowest usage among ${available.length} available limited models`;
  } else if (unlimited.length > 0) {
    selected = unlimited[0];
    reasoning = 'All limited models exhausted, using unlimited model';
  } else {
    const sorted = [...modelDetails].sort((a, b) => a.usagePercent - b.usagePercent);
    selected = sorted[0];
    reasoning = 'All models at capacity, using least-used as fallback';
  }
  
  return {
    model: selected.model,
    usagePercent: selected.usagePercent,
    reasoning,
    allModels: modelDetails,
  };
}

// Export the raw client for advanced usage
export { groqClient };

// Export model lists and limits
export { GROQ_MODEL_LIMITS, MODEL_PRIORITY, FAST_MODEL_PRIORITY };

// Default export for convenience
export default {
  createChatCompletion,
  complete,
  completeWithSystem,
  getUsageStatus,
  getTotalCapacity,
  getSelectedModel,
  GROQ_MODEL_LIMITS,
  MODEL_PRIORITY,
};
